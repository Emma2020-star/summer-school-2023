{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505e8f00-813a-4359-a06b-a47aedf90e2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![NASA logo](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/NASA_logo.svg/110px-NASA_logo.svg.png) ![IBM Research logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSwHxsDwxcOHsQUD2pghQ32j90pzsZLcOujpGCyU1yE&s)\n",
    "\n",
    "# Geospatial Foundation Model: Burn scar fine-tuning\n",
    "\n",
    "This is an example of how to fine-tune a model to map burn scars from HLS data using the IBM Geospatial Foundation models as a starting point.  \n",
    "\n",
    "To run a fine-tuning experiment for flood mapping we will use the MMSegmentation library (https://github.ibm.com/GeoFM-Finetuning/mmsegmentation) to fine-tune a model starting from the geospatial foundation model trained on HLS data.\n",
    "\n",
    "The following notebook assumes that you project files are placed in folder on the shared volume in the following folder structure:\n",
    "```\n",
    "configs                   - folder to place experiment configuration files\n",
    "fine-tune-checkpoints     - folder where training outputs will be generated\n",
    "GFM-Models                - folder containing the checkpoint files from the pre-trained GFM\n",
    "inference                 - folder where we will carry out our inference tasks\n",
    "training_data             - folder containing the training dataset (including labels and test/train splits etc)\n",
    "```\n",
    "\n",
    "You then create you configuration script, before submitting to the cluster to run.  The notebook will then guide you to: \n",
    "* monitor and visualise the training, \n",
    "* run the test tasks\n",
    "* use the trained model for local inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5989bd-51aa-4de6-a225-a93a25b9e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import string\n",
    "import sys\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "sys.path.append('../')\n",
    "import geoft\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Grab cluster details\n",
    "login_url, namespace, path_to_shared_volume = geoft.get_cluster_details()\n",
    "\n",
    "# Create S3 client (for pulling data and model weights)\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_access_key_secret = os.getenv(\"AWS_ACCESS_KEY_SECRET\")\n",
    "\n",
    "s3 = geoft.create_s3_client(aws_access_key_id, aws_access_key_secret)\n",
    "\n",
    "# S3 bucket where data and model weights reside\n",
    "bucket_name = \"nasa-gfm-summer-school\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404e1d6-32cc-43c8-ab64-9838f60606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(geoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec23b6a-8d55-4c7e-ab9a-3505279571fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Define the project name you wish to use\n",
    "\n",
    "project_name = \"burn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41713036-c595-457c-acfd-a82eb48cbc21",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "\n",
    "If we are starting a new fine-tuning project, we can create a new set of folders and download the training data+labels and the pre-train foundation model weights.  We create the folder structure described above, then pull the data and weights from an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d7d45-3695-43ac-8f22-65978f68dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Create project folder structure \n",
    "geoft.create_project_folders(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8e0ab-1b1a-4bd7-859a-a7aa710f137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Download the pre-trained model weights\n",
    "model_name = 'epoch-832-loss-0.0473.pt' # best for burn scar mapping\n",
    "s3.download_file(bucket_name, 'gfm-models/' + model_name, path_to_shared_volume + project_name + '/gfm-models/' + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7646968-6db7-46a4-9c9e-d6e9fe0ed53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- Download the training data\n",
    "dataset = 'burn-scars'\n",
    "\n",
    "training_data_path = path_to_shared_volume + project_name + '/training-data/'\n",
    "\n",
    "# Download training data\n",
    "subfolder = 'training/'\n",
    "geoft.download_s3_dir(dataset + '/' + subfolder, training_data_path, bucket_name, client=s3, number_of_files=200)\n",
    "\n",
    "# Download training data labels\n",
    "subfolder = 'validation/'\n",
    "geoft.download_s3_dir(dataset + '/' + subfolder, training_data_path, bucket_name, client=s3, number_of_files=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05d4b6-e6c1-4299-8346-42fa5e0f3169",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating fine-tuning configuration\n",
    "\n",
    "![Fine-tune architecture](../images/finetune_arch.png)\n",
    "\n",
    "\n",
    "To configure the fine-tuning, we define a configuration by creating a config file (example below).  You can edit the python scripts in the jupyterlab editor, in a cell and write with `%%writefile` or edit locally and upload. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39044e54-d093-4bed-992a-676a12ebd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'gfm_ckpt': 'epoch-832-loss-0.0473.pt',\n",
    "        'loss_function': '''type='DiceLoss', use_sigmoid=False, loss_weight=1''',\n",
    "        'batch_size': '4',\n",
    "        'learning_rate': '6e-5',\n",
    "        'aux_head': 'True',\n",
    "        'decode_head_conv': '1',        \n",
    "        'num_epochs': 50,\n",
    "        'number_training_files': 150,\n",
    "        'project_name': project_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd364c-1e64-4c70-9593-aaca3588ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name, experiment_filepath = geoft.generate_config(project_name, conf, \"burn_config.py.template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277ba77-fea7-4442-8610-3b65d0310256",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoft.view_config(experiment_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364b664-699f-41ab-9bed-809b19011734",
   "metadata": {},
   "source": [
    "## Submitting fine-tuning job to run\n",
    "\n",
    "The first thing you need to do in order to submit a training job to the cluster is login to the cluster.  This will only need to be done once per 24 hours.\n",
    "\n",
    "Run the cell below (`login_url`), and click on the generated url.\n",
    "\n",
    "Authenticate, then copy and paste the `oc login` command into the cell below (with `%%sh` at the top) and this will log you in to the cluster and allow you submit and monitor jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7c5c8-1243-48f7-848e-31158ad49591",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774ba1a-b7d3-40ee-9c51-0118206d030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "oc login --token=sha256~fw9M6wcxufG5aqBv7IcLkRj7Z0aMKetpqTiajOc4yg4 --server=https://api.codeflare.xx6d.p1.openshiftapps.com:6443\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df8d4d-3309-4a28-9565-cb1afa534a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcad_id = geoft.submit_tune(project_name,\n",
    "                namespace,\n",
    "                experiment_name,\n",
    "                image='quay.io/bedwards-ibm/mmsegmentation-geo:latest',\n",
    "                num_gpus=1,\n",
    "                memory_mb=28000)\n",
    "\n",
    "print(mcad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195bd999-0402-4545-8c22-742584326926",
   "metadata": {},
   "source": [
    "## Monitoring training job\n",
    "Once you have submitted the job to the cluster, we can monitor it using the following commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47827ce7-38e2-4a6a-9a29-23c769dd3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "torchx list -s kubernetes_mcad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b36f8c-6b41-4cc5-bd66-ed9ef495c7ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_log_cmd = '''torchx log ''' + str(mcad_id) +  ''' | tail -n20'''\n",
    "os.system(check_log_cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009687b5-9f81-4c35-9bb0-3adb9963b9dc",
   "metadata": {},
   "source": [
    "## Viewing the training metrics\n",
    "\n",
    "Now that we have run (or at least are running) the experiment, we can view the training metrics.  To do this we will load the log file and extract the metrics to a dataframe (`val_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a922b-6f56-4d65-bb39-af505eb609b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = geoft.load_tune_metrics(project_name, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66741529-f6a5-4a78-8cfb-aea06f0772e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_figwidth(15)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_df.index, train_df.loss, '-r');\n",
    "plt.ylabel('Training Loss');\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_df.index, train_df.loss_val, '-b');\n",
    "plt.ylabel('Validation Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab586-cfa2-439e-b0aa-97f8308b401d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10de3e8-7958-44ca-9ff4-9ceef79f4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mcad_id = geoft.submit_test(project_name,\n",
    "                        namespace,\n",
    "                        experiment_name,\n",
    "                        checkpoint='latest.pth',\n",
    "                        num_gpus=1,\n",
    "                        memory_mb=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b22b5-5b91-49db-9a43-748c27c70571",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = geoft.get_test_metrics(project_name, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b7fe89-5f24-46ee-bc99-4dc02ac3857b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running inference using the trained model\n",
    "\n",
    "Once we have a trained model, we can use it to run inference on other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb2fee-684f-44bb-aa3e-790f69aa0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_mcad_id = geoft.submit_inference(project_name,\n",
    "                namespace,\n",
    "                experiment_name,\n",
    "                checkpoint='latest.pth',\n",
    "                image='quay.io/bedwards-ibm/mmsegmentation-geo:latest',\n",
    "                num_gpus=1,\n",
    "                memory_mb=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0c875-ebe5-4d4e-a3cf-a842b313f145",
   "metadata": {},
   "source": [
    "## Visualizing the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f63a4-6167-47d6-aeef-e10036095826",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a65c3d-df4a-4c8f-a5be-c5d511333cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins as plugins\n",
    "import numpy as np\n",
    "\n",
    "def colorize(array, cmax, cmin=0, cmap=\"rainbow\"):\n",
    "    \"\"\"Converts a 2D numpy array of values into an RGBA array given a colour map and range.\n",
    "    Args:\n",
    "        array (ndarray):\n",
    "        cmax (float): Max value for colour range\n",
    "        cmin (float): Min value for colour range\n",
    "        cmap (string): Colour map to use (from matplotlib colourmaps)\n",
    "    Returns:\n",
    "            rgba_array (ndarray): 3D RGBA array which can be plotted.\n",
    "    \"\"\"\n",
    "    normed_data = (array - cmin) / (array.max() - cmin)\n",
    "    cm = plt.cm.get_cmap(cmap)\n",
    "    return cm(normed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cbd93-9ab5-4dc2-8c60-67f6a6cfb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_files = sorted(glob.glob('/opt/app-root/src/data/' + project_name + '/inference/*.tif'))\n",
    "inference_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ef2e4-cdf6-495a-8b1b-e6792b1e0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenum = 0\n",
    "original_file = inference_files[filenum]\n",
    "predict_file = inference_files[filenum].replace('/inference','/inference/pred/' + experiment_name).replace('.tif','_pred.tif')\n",
    "\n",
    "# Load the original image layer\n",
    "with rasterio.open(original_file) as src:\n",
    "    redArray = src.read(1)\n",
    "    greenArray = src.read(2)\n",
    "    blueArray = src.read(3)\n",
    "    bounds = src.bounds\n",
    "    nd = src.nodata\n",
    "    midLat = (bounds[3] + bounds[1]) / 2\n",
    "    midLon = (bounds[2] + bounds[0]) / 2\n",
    "    im_rgb = np.moveaxis(np.array([redArray,greenArray,blueArray]), 0, -1)/2048\n",
    "    # im_rgb = im_rgb/np.max(im_rgb)\n",
    "\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(location=[midLat, midLon], tiles='openstreetmap', max_zoom=22)\n",
    "\n",
    "# Add the prediciton layer to the map\n",
    "with rasterio.open(predict_file) as src:\n",
    "    dataArray = src.read(1)\n",
    "    bounds = src.bounds\n",
    "    nd = src.nodata\n",
    "\n",
    "# cmax = np.max(dataArray)\n",
    "cmax = 1000\n",
    "dataArrayMasked = np.ma.masked_where(dataArray == nd, dataArray)\n",
    "dataArrayMasked = np.ma.masked_where(dataArray == 0, dataArrayMasked)\n",
    "imc = colorize(dataArrayMasked, cmax, cmin=0, cmap=\"viridis\")\n",
    "\n",
    "# Add the layers to the map\n",
    "pred = folium.raster_layers.ImageOverlay(imc, [[bounds[1], bounds[0]], [bounds[3], bounds[2]]], name=\"Prediction\", opacity=0.8)\n",
    "orig = folium.raster_layers.ImageOverlay(im_rgb, [[bounds[1], bounds[0]], [bounds[3], bounds[2]]], name=\"Original image\", opacity=1.0)\n",
    "\n",
    "orig.add_to(m)\n",
    "pred.add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m.fit_bounds(bounds)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97434603-c419-40c1-b0c7-2bbbdb23417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc50ab-acb6-4094-8292-cde9b108ba48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774037e9-0061-4b03-953d-a1031a6468c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fc6c6-33dc-443e-b96c-08aef3758227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
